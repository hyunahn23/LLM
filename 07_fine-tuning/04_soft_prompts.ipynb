{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydxwPZkP8z7S"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import get_peft_model, PromptTuningConfig\n",
        "import torch"
      ],
      "metadata": {
        "id": "sYHPgKjv9z-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "yRAvHq2TBY-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotion_tokens = ['<happy>', '<sad>']\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('gpt2', token=HF_TOKEN)\n",
        "tokenizer.add_special_tokens({\"additional_special_tokens\": emotion_tokens})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCAJMHEb-DVf",
        "outputId": "5f8a0fa4-4a9c-456f-95d9-c82275b7a0b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = [\n",
        "    ('<happy> Once upon a time, there was a dragon who',\n",
        "     'The dragon breathed colorful fireworks that lit up the sky.'),\n",
        "    ('<sad> In a dark forest, a lonely knight',\n",
        "     'The Knight knelt by the withered tree, tears falling on his rusted armor.')\n",
        "]"
      ],
      "metadata": {
        "id": "OS4qoEuJ_-9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\", token=HF_TOKEN).to(device)"
      ],
      "metadata": {
        "id": "pgwC2GD0BFGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.resize_token_embeddings(len(tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdCvIaO-DN99",
        "outputId": "8e466b17-83c3-4913-a586-340f4c2ae3f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(50259, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config = PromptTuningConfig(\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    num_virtual_tokens=10,\n",
        "    token_dim=model.config.hidden_size\n",
        ")\n",
        "print(model.config.hidden_size)\n",
        "\n",
        "model = get_peft_model(model, peft_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fI0lY6cuD_jS",
        "outputId": "bffd9749-eec2-4a68-c465-980fbdcb3c68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "768\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "\n",
        "for epoch in range(30):\n",
        "  for prompt, continuation in train_data:\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    labels = tokenizer(continuation, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "    full_inputs = torch.cat([inputs.input_ids, labels], dim=1)\n",
        "    outputs = model(full_inputs, labels=full_inputs)\n",
        "\n",
        "    loss = outputs.loss\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbiVTnKgGF8Y",
        "outputId": "8c4c15d6-6df8-4a67-b3a2-220dd7a8a250"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_story(emotion, prompt):\n",
        "  inputs = tokenizer(f'{emotion} {prompt}', return_tensors=\"pt\").to(device)\n",
        "\n",
        "  output = model.generate(\n",
        "      **inputs,\n",
        "      max_new_tokens=50,\n",
        "      temperature=0.9,\n",
        "      top_k=40,\n",
        "      repetition_penalty=1.5,\n",
        "      do_sample=True\n",
        "  )\n",
        "\n",
        "  return tokenizer.decode(output[0], skip_special_tokens=False)"
      ],
      "metadata": {
        "id": "bMoOEwxTOC14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_story('<happy>', 'In a magical kingdom'))\n",
        "print(\"=\" * 100)\n",
        "print(generate_story('<sad>', 'In a magical kingdom'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BaQ8KBcQOFS",
        "outputId": "7c268e3f-33ad-4daa-a43a-39d34c8d2f2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.11/dist-packages/peft/peft_model.py:1926: UserWarning: Position ids are not supported for parameter efficient tuning. Ignoring position ids.\n",
            "  warnings.warn(\"Position ids are not supported for parameter efficient tuning. Ignoring position ids.\")\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<happy> In a magical kingdom for the last\n",
            " I think he, as of it's about to be. \"I would you and all that she was by his (if this is not so much my will go no use in making up just one day,\" which way on how\n",
            "====================================================================================================\n",
            "<sad> In a magical kingdom. ( for an all \" in the number of people, and as we have to do not even be on that it is\n",
            "<|endoftext|>\n"
          ]
        }
      ]
    }
  ]
}